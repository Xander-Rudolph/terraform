name: Call LLaMA 3.1-405B

on:
  pull_request:
    branches:
      - main

jobs:
  call-llama:
    runs-on: ubuntu-latest
    container: ollama/ollama:latest

    services:
      ollama-model:
        image: ollama/ollama:latest
        volumes:
          - ./llama_model:/app/model

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Hugging Face CLI
        run: pip install huggingface-cli

      - name: Authenticate with Hugging Face
        run: huggingface-cli login
        env:
          HUGGINGFACE_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}

      - name: Download Meta-Llama-3.1-405B model
        run: huggingface-cli repo download meta-llama/Meta-Llama-3.1-405B --local-dir ./llama_models

      - name: Call LLaMA model with branch contents
        env:
          BRANCH_CONTENTS: ${{ github.event.pull_request.body }}
        run: |
          ollama --model_path ./llama_model --model_name llama-3.1-405B --prompt "$BRANCH_CONTENTS" > output.txt

      - name: Comment on PR with results
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.issues.createComment({
              issue_number: github.context.pullRequest.number,
              owner: github.context.repo.owner,
              repo: github.context.repo.repo,
              body: `LLaMA 3.1-405B response: \n\`\`\`${fs.readFileSync('output.txt', 'utf8')}\`\`\``
            })
