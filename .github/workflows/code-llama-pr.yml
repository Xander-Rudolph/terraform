name: AI Pull Review

on:
  pull_request:
  workflow_call:

jobs:
  review-pr:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
    services:
      ollama-model:
        image: ollama/ollama:latest
        volumes:
          - ./llama_model:/app/model
        ports:
          - 11434:11434
    steps:
      - name: Send PR to ollama
        shell: pwsh
        id: review
        run: |
          $Uri = "http://localhost:11434"
          $model = 'llama3.1'
          $prompt = '${{ github.event.pull_request.body }}'
          $ErrorActionPreference = 'Inquire'
          $VerbosePreference = 'continue'

          Write-Verbose "Test if ollama is responding..."
          Try {
              $testService = invoke-restmethod -uri $Uri -Method get
          }
          catch {
              Write-Error "Ollama didn't respond...`n$($_.exception)"
              exit
          }
          Write-Verbose "Response:`n$testService"

          # format URI for API calls
          $baseUri = "$Uri/api"

          # Declare default splat
          $splat = @{
              Headers = @{
                  "Content-Type" = "application/json"
              }
              Method = "Post"
          }

          $splat.body = @{
              name = $model
          } | ConvertTo-Json
          Try {
              $pullModel = Invoke-WebRequest @splat -Uri "$baseUri/pull"
          }
          catch {
              Write-Error "Failed to pull model...`n$($_.exception)"
              exit
          }

          # Use Invoke-WebRequest to send the question
          $body = @{
              prompt = $prompt
              stream = $false
              model = $model
          }
          $splat.Body = $body | ConvertTo-Json
          $response = Invoke-WebRequest @splat -Uri "$baseUri/generate"

          $comment = ($response.content | ConvertFrom-Json).response
          echo "comment=$comment >> "$GITHUB_OUTPUT"
          
      - name: Comment on PR with results
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const output = `#### PR review ⚙️\`${{ steps.review.outcome }}\`
            <details><summary>Summary</summary>
            \`\`\`\n
            ${{ steps.review.outputs.comment }}
            \`\`\`
            </details>
            
            *Pusher: @${{ github.actor }}, Workflow: \`${{ github.workflow }}\`*`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })
            
      ## the commened out section was generated with ollama... so... humans are still better
      # - name: Set up Python
      #   uses: actions/setup-python@v4
      #   with:
      #     python-version: '3.x'

      # - name: Install Hugging Face transformers
      #   run: |
      #     python -m ensurepip
      #     python -m pip install --upgrade pip
      #     python -m pip install transformers

      # - name: Authenticate with Hugging Face
      #   run: |
      #     huggingface-cli login --token ${{ secrets.HUGGING_FACE_TOKEN }}

      # - name: Download Meta-Llama-3.1-405B model
      #   run: huggingface-cli repo download meta-llama/Meta-Llama-3.1-405B --local-dir ./llama_models

      # - name: Call LLaMA model with branch contents
      #   env:
      #     BRANCH_CONTENTS: ${{ github.event.pull_request.body }}
      #   run: |
      #     ollama --model_path ./llama_model --model_name llama-3.1-405B --prompt "$BRANCH_CONTENTS" > output.txt

      # - name: Comment on PR with results
      #   uses: actions/github-script@v6
      #   with:
      #     github-token: ${{ secrets.GITHUB_TOKEN }}
      #     script: |
      #       github.issues.createComment({
      #         issue_number: github.context.pullRequest.number,
      #         owner: github.context.repo.owner,
      #         repo: github.context.repo.repo,
      #         body: `LLaMA 3.1-405B response: \n\`\`\`${fs.readFileSync('output.txt', 'utf8')}\`\`\``
      #       })


